---
title: "Initial Project Notes"
subtitle: "Scraping a Page"
output: html_notebook
---

# First Draft
-   I selected IG I<sup>2</sup> 1086 as the first inscription to be scraped
-   It was very successful!

```{r IG I2 1086}
source("ScrapingPackhum/IG I2 1086.r")
```

-   Outputs are:
    -   the book
    -   the inscription number
    -   the Packard Humanities internal reference number
    -   the text
    
## Breakdown
-   The script performs five distinct functions
    -   Scraping the target website
    -   Finding the book identifiers
    -   Finding the PackHum identifier
    -   Finding the text
    -   Printing the output

## 0) Setup
The script as is calls two packages: `dplyr` and `rvest`.
`dplyr` is used for data manipulation and `rvest` for web scraping.

## 1) Target Website
The script begins by turning the target website into the variable `link`.
This is then passed through `read_html()`, which scrapes the target webpage.
The output is stored as the variable (web)`page`.

```{r GRAB PAGE, eval=FALSE, include=TRUE}
### GRAB PAGE ------------------------------------------------------------------
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
page <- read_html(link)
```

## 2) Book Identifiers
The script then needs to store which inscription it is scraping.
This info has two components: the volume of the Inscriptiones Graecae, and the
inscription number.

First, the volume. The command `html_nodes()` searches the `page` for the volume
using its selector path. This is copied from the website using Safari's
developer panel. The minimal selector path for this volume is
`"span.fullref > a"`. This data is then cleaned of any html tags using
`html_text()`. Escape characters, specifically the newline character `\n`, are
removed using Regular Expression and the command `gsub()`. The clean text which
results is stored as the variable `IGbook`.

The inscription number is selected by the same method, using the selector path
`"span.fullref > span"`. The data is stored as the variable `IGno`.

```{r GRAB TEXT REF, eval=FALSE, include=TRUE}
### GRAB TEXT REF --------------------------------------------------------------
#textpage > div.hdr2 > span.fullref > a
IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
#textpage > div.hdr2 > span.fullref > span  
IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
```

## 3) Packard Humanities Identifier
Every inscription in the Packard Humanities Institute library includes a unique
reference number. For the purposes of the study, this number fills two roles.
First, since each number is unique, it may be used as a global identifier
in contrast to the local identifiers found in §2. Every volume of the
Inscriptiones Graecae will contain an inscription called 1, and 2, etc. These
may be distinguished using the volume number, or by using the PHI inscription
number. For example, the inscription IG I<sup>2</sup> 1086 is the only one with
the PHI inscription number PH1825.

Second, this reference number is included in the web address. See the link:

```{r LINK CHECK, eval=FALSE, include=TRUE}
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
```

The number `1825` is indeed the PHI inscription number,
with the prefix "PH" removed. The result is that this number may be used to
systematically scrape every inscription from the PHI website. This idea is to
be tested next.

The PHI inscription number is grabbed in the same manner as `IGbook` and
`IGno`, with the cleaned data stored as the variable `PHIno`.

```{r GRAB PACKHUM REF, eval=FALSE, include=TRUE}
### GRAB PACKHUM REF -----------------------------------------------------------
#textpage > div.docref
PHIno <- page %>% html_nodes('div.docref') %>% html_text()
  PHIno <- gsub("\\n", "", PHIno)
```

## 4) The Inscription
The inscription is scraped in the same manner as the past three variables.
The major difference is in the cleaning step: the inscriptions often contain
editorial marks which must be removed before any analysis.

The list of marks to be removed will be updated as more arise.


```{r GRAB TEXT, eval=FALSE, include=TRUE}
### GRAB TEXT ------------------------------------------------------------------
#textpage > div.text > div.greek.text-nowrap.dblclk > table
text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("\\n", "", text)
  text <- gsub("\\d", "", text)
  text <- gsub("-", "", text)
  #ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text)
```

# Second Draft
The script currently prints the scraped data onto the console. This is fine when
working with a single inscription, but will quickly become unusable when
scraping more pages. The variables `IGbook`, `IGno`, `PHIno`, and `text` should
be redefined as lists. 

Since I'm planning ahead, I suppose I should wrap the scraping into functions.

## Designing the Loop
The Packard Humanities Reference Number is *also the identifying portion of the
web address!* Everything after the ampsersand in the above address, 
`https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701`,
is added while navigating through the website's tree of variables
(volume, region, etc.). The script can therefore be built around a `for()` loop
which α) builds a new variable for each loop and β) checks to ensure the
resulting web address corresponds to a valid PHI Reference Number.

## The Function
The loop is built around a function, currently called `TEST()`, which captures
and organizes the data for us.

```{r FUNCTION TEST, eval=FALSE, include=TRUE}
TEST <- function(PHIno = 20) {
  link <- paste('https://inscriptions.packhum.org/text/', PHIno, sep="")
  page <- read_html(link)
  
  ### Text
  #textpage > div.text > div.greek.text-nowrap.dblclk > table
  text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("[\\d\\[\\]]", "", text, perl=T)
  text <- gsub("(\\-\\n{1,})", "", text, perl=T)
  text <- gsub("\\n{1,}", "", text, perl=T)
  text <- gsub("\\s{1,}", " ", text, perl=T)
  # ̣COMBINING DOT BELOW Unicode: U+0323, UTF-8: CC A3
  text <- gsub("̣", "", text, perl=T)
  text <- gsub("#", "", text, perl=T)
  # ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text, perl=T)
  
  ### IGBook
  #textpage > div.hdr2 > span.fullref > a
  IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
  ### IGNo
  IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
  
  # Atm keeping for debugging
  # print(link)
  # print(text)
  # print(IGbook)
  # print(IGno)
  # print(PHIno)
  
  # Make entry to be returned
  entry <- c(IGbook, IGno, PHIno, text, link)
  return(as.list(entry))
  
  # Atm keeping for debugging
  # rm(link)
  # rm(text)
  # rm(IGbook)
  # rm(IGno)
  # rm(PHIno)
}
```

This essentially encapsulates the contents of the first draft. The main
differences are in the formatting changes (more notational marks to be removed)
and how it handles the output. Rather than printing off variables to the
terminal, the function creates a vector called `entry`, which it returns as a
list for later processing. The target `PHIno` is now  also identified as an
argument of the function. The default `PHIno` is 20 (IG I^3 20), because it's an
eyesore and I'm a masochist. Or because I slammed it in at random. WHo's to say.

There are also a few test lines at the bottom that use the `TEST()` function in
a `for()` loop to gather the first 100 inscriptions (by `PHIno`) from the
PackHum website.

```{r FIRST 100, eval=FALSE, include=TRUE}
# df[row, column]
test_df <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_df[1,] <- TEST(1)
test_df[2,] <- TEST(2)
test_df[3,] <- TEST(3)

for (i in c(1:100)) {
  test_df[i,] <- TEST(i)
}
```

```{r DISPLAY FIRST 100, echo=FALSE}
head(test_df)
```

## Bad Selector Paths
Now, trying to scan a non-existent page will obviously break things. An example
from the draft:

```{r BAD SELECTORS}
# testing bad selector paths ---------------------------------------------------
test_prime <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_prime[1,] <- TEST(0)
```

Two solutions come to mind. 

I could create a list of hypothetical selectors between 1 and the presumed
highest `PHIno`, which I then systematically "ping" to find the rejects. Running
`TEST(0)` will only return two outputs: the `IGbook` and the `link`.

```{r BAD OUTPUTS}
TEST(0)
```

This means there is certainly a path available where I run a `for()` loop, which
tests all selectors between 1 and *n* and sets their `IGbook` output into a
tibble. From there, any 0's can be dropped.

There is also the option where I manually define the ranges which I intend to
gather. This would avoid *scanning the Inscriptiones Graecae in its entirety*,
but would raise the chances of missing potential data.

