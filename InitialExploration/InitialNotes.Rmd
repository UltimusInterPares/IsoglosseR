---
title: "Initial Project Notes"
subtitle: "Scraping a Page"
output: html_notebook
---


# First Draft
-   I selected IG I<sup>2</sup> 1086 as the first inscription to be scraped
-   It was very successful!

```{r IG I2 1086}
source("ScrapingPackhum/IG I2 1086.r")
```

-   Outputs are:
    -   the book
    -   the inscription number
    -   the Packard Humanities internal reference number
    -   the text
    
## Breakdown
-   The script performs five distinct functions
    -   Scraping the target website
    -   Finding the book identifiers
    -   Finding the PackHum identifier
    -   Finding the text
    -   Printing the output

## 0) Setup
The script as is calls two packages: `dplyr` and `rvest`.
`dplyr` is used for data manipulation and `rvest` for web scraping.

## 1) Target Website
The script begins by turning the target website into the variable `link`.
This is then passed through `read_html()`, which scrapes the target webpage.
The output is stored as the variable (web)`page`.

```{r GRAB PAGE, eval=FALSE, include=TRUE}
### GRAB PAGE ------------------------------------------------------------------
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
page <- read_html(link)
```

## 2) Book Identifiers
The script then needs to store which inscription it is scraping.
This info has two components: the volume of the Inscriptiones Graecae, and the
inscription number.

First, the volume. The command `html_nodes()` searches the `page` for the volume
using its selector path. This is copied from the website using Safari's
developer panel. The minimal selector path for this volume is
`"span.fullref > a"`. This data is then cleaned of any html tags using
`html_text()`. Escape characters, specifically the newline character `\n`, are
removed using Regular Expression and the command `gsub()`. The clean text which
results is stored as the variable `IGbook`.

The inscription number is selected by the same method, using the selector path
`"span.fullref > span"`. The data is stored as the variable `IGno`.

```{r GRAB TEXT REF, eval=FALSE, include=TRUE}
### GRAB TEXT REF --------------------------------------------------------------
#textpage > div.hdr2 > span.fullref > a
IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
#textpage > div.hdr2 > span.fullref > span  
IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
```

## 3) Packard Humanities Identifier
Every inscription in the Packard Humanities Institute library includes a unique
reference number. For the purposes of the study, this number fills two roles.
First, since each number is unique, it may be used as a global identifier
in contrast to the local identifiers found in §2. Every volume of the
Inscriptiones Graecae will contain an inscription called 1, and 2, etc. These
may be distinguished using the volume number, or by using the PHI inscription
number. For example, the inscription IG I<sup>2</sup> 1086 is the only one with
the PHI inscription number PH1825.

Second, this reference number is included in the web address. See the link:

```{r LINK CHECK, eval=FALSE, include=TRUE}
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
```

The number `1825` is indeed the PHI inscription number,
with the prefix "PH" removed. The result is that this number may be used to
systematically scrape every inscription from the PHI website. This idea is to
be tested next.

The PHI inscription number is grabbed in the same manner as `IGbook` and
`IGno`, with the cleaned data stored as the variable `PHIno`.

```{r GRAB PACKHUM REF, eval=FALSE, include=TRUE}
### GRAB PACKHUM REF -----------------------------------------------------------
#textpage > div.docref
PHIno <- page %>% html_nodes('div.docref') %>% html_text()
  PHIno <- gsub("\\n", "", PHIno)
```

## 4) The Inscription
The inscription is scraped in the same manner as the past three variables.
The major difference is in the cleaning step: the inscriptions often contain
editorial marks which must be removed before any analysis.

The list of marks to be removed will be updated as more arise.


```{r GRAB TEXT, eval=FALSE, include=TRUE}
### GRAB TEXT ------------------------------------------------------------------
#textpage > div.text > div.greek.text-nowrap.dblclk > table
text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("\\n", "", text)
  text <- gsub("\\d", "", text)
  text <- gsub("-", "", text)
  #ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text)
```

# Second Draft
The script currently prints the scraped data onto the console. This is fine when
working with a single inscription, but will quickly become unusable when
scraping more pages. The variables `IGbook`, `IGno`, `PHIno`, and `text` should
be redefined as lists. 

Since I'm planning ahead, I suppose I should wrap the scraping into functions.

## Designing the Loop
The Packard Humanities Reference Number is *also the identifying portion of the
web address!* Everything after the ampsersand in the above address, 
`https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701`,
is added while navigating through the website's tree of variables
(volume, region, etc.). The script can therefore be built around a `for()` loop
which α) builds a new variable for each loop and β) checks to ensure the
resulting web address corresponds to a valid PHI Reference Number.

## The Function
The loop is built around a function, currently called `TEST()`, which captures
and organizes the data for us.

```{r FUNCTION TEST, eval=FALSE, include=TRUE}
TEST <- function(PHIno = 20) {
  link <- paste('https://inscriptions.packhum.org/text/', PHIno, sep="")
  page <- read_html(link)
  
  ### Text
  #textpage > div.text > div.greek.text-nowrap.dblclk > table
  text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("[\\d\\[\\]]", "", text, perl=T)
  text <- gsub("(\\-\\n{1,})", "", text, perl=T)
  text <- gsub("\\n{1,}", "", text, perl=T)
  text <- gsub("\\s{1,}", " ", text, perl=T)
  # ̣COMBINING DOT BELOW Unicode: U+0323, UTF-8: CC A3
  text <- gsub("̣", "", text, perl=T)
  text <- gsub("#", "", text, perl=T)
  # ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text, perl=T)
  
  ### IGBook
  #textpage > div.hdr2 > span.fullref > a
  IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
  ### IGNo
  IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
  
  # Atm keeping for debugging
  # print(link)
  # print(text)
  # print(IGbook)
  # print(IGno)
  # print(PHIno)
  
  # Make entry to be returned
  entry <- c(IGbook, IGno, PHIno, text, link)
  return(as.list(entry))
  
  # Atm keeping for debugging
  # rm(link)
  # rm(text)
  # rm(IGbook)
  # rm(IGno)
  # rm(PHIno)
}
```

This essentially encapsulates the contents of the first draft. The main
differences are in the formatting changes (more notational marks to be removed)
and how it handles the output. Rather than printing off variables to the
terminal, the function creates a vector called `entry`, which it returns as a
list for later processing. The target `PHIno` is now  also identified as an
argument of the function. The default `PHIno` is 20 (IG I^3 20), because it's an
eyesore and I'm a masochist. Or because I slammed it in at random. WHo's to say.

There are also a few test lines at the bottom that use the `TEST()` function in
a `for()` loop to gather the first 100 inscriptions (by `PHIno`) from the
PackHum website.

```{r FIRST 100, eval=FALSE, include=TRUE}
# df[row, column]
test_df <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_df[1,] <- TEST(1)
test_df[2,] <- TEST(2)
test_df[3,] <- TEST(3)

for (i in c(1:100)) {
  test_df[i,] <- TEST(i)
}
```

```{r DISPLAY FIRST 100, echo=FALSE}
head(test_df)
```

## Bad Links
Now, trying to scan a non-existent page will obviously break things. An example
from the draft:

```{r BAD LINKS}
# testing bad links ------------------------------------------------------------
test_prime <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_prime[1,] <- TEST(0)
```

Two solutions come to mind. 

I could create a list of hypothetical selectors between 1 and the presumed
highest `PHIno`, which I then systematically "ping" to find the rejects. Running
`TEST(0)` will only return two outputs: the `IGbook` and the `link`.

```{r BAD OUTPUTS}
TEST(0)
```

This means there is certainly a path available where I run a `for()` loop, which
tests all selectors between 1 and *n* and sets their `IGbook` output into a
tibble. From there, any 0's can be dropped.

There is also the option where I manually define the ranges which I intend to
gather. This would avoid *scanning the Inscriptiones Graecae in its entirety*,
but would raise the chances of missing potential data.

---

After some thought, I'm of the opinion that I should use a function for link
validation. Assuming the code works, a programmatic approach should
outperform a manual search by every metric. This makes sense! It's a repetitive
task that would normally take many long hours to complete. It's everything a
computer excels at!

What's more, the output of the loop could be used as the limiter for any
`for()` loops, improving the function's control and reproducibility.


<style type="text/css">
.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36;
  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3;
  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax"></th>
    <th class="tg-baqh">Automated</th>
    <th class="tg-baqh">Manual</th>
    <th class="tg-0lax">Assuming the code works …</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Time (Coding)</td>
    <td class="tg-baqh">-</td>
    <td class="tg-baqh">+</td>
    <td class="tg-0lax">Manual isn't coded</td>
  </tr>
  <tr>
    <td class="tg-0lax">Time (Search)</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Automated does it for you</td>
  </tr>
  <tr>
    <td class="tg-0lax">Output as Obj.</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Output can be used as `i`</td>
  </tr>
  <tr>
    <td class="tg-0lax">Thematic</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Paper is about programming</td>
  </tr>
  <tr>
    <td class="tg-0lax">Missed Links</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code will try every possible option</td>
  </tr>
  <tr>
    <td class="tg-0lax">Volume per TIme</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code will scan more in same time</td>
  </tr>
  <tr>
    <td class="tg-0lax">False pos/neg</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code won't mistake a 0 for other `n`</td>
  </tr>
</tbody>
</table>


The matter of time brings up another point. Currently, I have no estimate for
how long anything will take to run. While this isn't a problem for smaller code
chunks, it may become a major factor in the full-scale scraper/miner. My laptop
isn't built to handle that kind of network or memory load, and I can't use the
GAI servers without my Dell, so this *needs to work* when I scale up. I
literally cannot run this repeatedly, I just do not have that kind of time on my
hands.

Time benchmarking is a faster fix, so that'll come first.

## Time Benchmarks
There are two options for measuring time elapsed during execution.

    1       Sys.time()
    2       system.time(FUNC())

`Sys.time()` returns the current time.
```{r sys.time()}
Sys.time()
```

`system.time(FUNC())` returns the execution time of some arbitrary function
FUNC(). It looses the output of the argument.

```{r system.time()}
system.time(TEST(200))
```

Current plan. Capture a timestamp at the beginning of execution. Capture another
at the end. Find the difference!
```{r Time Example}
# Capture first time stamp
test_time_a <- Sys.time()

# Wait for 20 seconds
Sys.sleep(5)

# Capture last time stamp
test_time_b <- Sys.time()

# Find difference
test_time_b - test_time_a
```

Now, adding `Sys.time()` directly to the function seems unnecessary. I don't
want to complicate things by adding more outputs to a function, plus I don't
really care how long it takes to capture the individual page. I'm more
interested in how long it takes to complete the full routine. Benchmarking
should then take place at the highest possible level.

## Time Estimating
I can, however, use `system.time()` in a loop to estimate how long a full-scale
execution might look. I can't do it quite yet — I don't have the upper bounds
that I'm estimating for — but I can keep it in my pocket.

# Third Draft
The project having been delayed for some time now, I find myself *overwhelmingly
confused by my lack of organization* (shocker)

Here's a rough outline of how the code should break down.

```{r Scraper Outline, eval=FALSE}
#phi_no is known in advance, or used to iterate the loop
#no need to define it, just add it into the mix during MakeEntry()
#idk what to do about the header yet so
Scrape(phi_no=1, dirty=F, remove_accent=T) =>

  # Find page for scraping
  page <- MakePage(phi_no)
  
  # Scrape relevant data + identifying materials
  text <- ReadText(page)
  ig_book <- ReadBook(page)
  ig_no <- ReadNo(page)
  header <- ReadHeader(page)
  
  # Clean data of editorial marks
  if (dirty=F) {
    text <- Clean(text)
  }
  
  # Clean data of accentual marks
  if (remove_accent=T) {
    no_accent <- RemoveAccent(text)
  } else {
    no_accent <- NA
  }
  
  # Make entry to be passed to tibble `data`
  entry <- MakeEntry(ig_book, ig_no, phi_no, text, duplicate, link)
  Return(entry)
  
# ---------------------------------------------------------
# Tibble `data` to be processed
data <- tibble(ig_book = NA, ig_no = NA, phi_no = NA, text = NA, duplicate = NA, link = NA)

# --------------------------------------------------------- 
# Master loop which creates the tibble
CollectData() =>
  for (i in c(1:n)) {
    data[i,] <- Scrape(phi_no, dirty=F, duplicate=T)
  }
```


Okay it's going to take 13 hours to run. So. Let's run a test scrape.

I'm going to try and grab all of Book 7...