---
title: "Initial Project Notes"
subtitle: "Scraping a Page"
output: html_notebook
---


# First Draft
-   I selected IG I<sup>2</sup> 1086 as the first inscription to be scraped
-   It was very successful!

```{r IG I2 1086}
source("ScrapingPackhum/IG I2 1086.r")
```

-   Outputs are:
    -   the book
    -   the inscription number
    -   the Packard Humanities internal reference number
    -   the text
    
## Breakdown
-   The script performs five distinct functions
    -   Scraping the target website
    -   Finding the book identifiers
    -   Finding the PackHum identifier
    -   Finding the text
    -   Printing the output

## 0) Setup
The script as is calls two packages: `dplyr` and `rvest`.
`dplyr` is used for data manipulation and `rvest` for web scraping.

## 1) Target Website
The script begins by turning the target website into the variable `link`.
This is then passed through `read_html()`, which scrapes the target webpage.
The output is stored as the variable (web)`page`.

```{r GRAB PAGE, eval=FALSE, include=TRUE}
### GRAB PAGE ------------------------------------------------------------------
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
page <- read_html(link)
```

## 2) Book Identifiers
The script then needs to store which inscription it is scraping.
This info has two components: the volume of the Inscriptiones Graecae, and the
inscription number.

First, the volume. The command `html_nodes()` searches the `page` for the volume
using its selector path. This is copied from the website using Safari's
developer panel. The minimal selector path for this volume is
`"span.fullref > a"`. This data is then cleaned of any html tags using
`html_text()`. Escape characters, specifically the newline character `\n`, are
removed using Regular Expression and the command `gsub()`. The clean text which
results is stored as the variable `IGbook`.

The inscription number is selected by the same method, using the selector path
`"span.fullref > span"`. The data is stored as the variable `IGno`.

```{r GRAB TEXT REF, eval=FALSE, include=TRUE}
### GRAB TEXT REF --------------------------------------------------------------
#textpage > div.hdr2 > span.fullref > a
IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
#textpage > div.hdr2 > span.fullref > span  
IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
```

## 3) Packard Humanities Identifier
Every inscription in the Packard Humanities Institute library includes a unique
reference number. For the purposes of the study, this number fills two roles.
First, since each number is unique, it may be used as a global identifier
in contrast to the local identifiers found in §2. Every volume of the
Inscriptiones Graecae will contain an inscription called 1, and 2, etc. These
may be distinguished using the volume number, or by using the PHI inscription
number. For example, the inscription IG I<sup>2</sup> 1086 is the only one with
the PHI inscription number PH1825.

Second, this reference number is included in the web address. See the link:

```{r LINK CHECK, eval=FALSE, include=TRUE}
link <- 'https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701'
```

The number `1825` is indeed the PHI inscription number,
with the prefix "PH" removed. The result is that this number may be used to
systematically scrape every inscription from the PHI website. This idea is to
be tested next.

The PHI inscription number is grabbed in the same manner as `IGbook` and
`IGno`, with the cleaned data stored as the variable `PHIno`.

```{r GRAB PACKHUM REF, eval=FALSE, include=TRUE}
### GRAB PACKHUM REF -----------------------------------------------------------
#textpage > div.docref
PHIno <- page %>% html_nodes('div.docref') %>% html_text()
  PHIno <- gsub("\\n", "", PHIno)
```

## 4) The Inscription
The inscription is scraped in the same manner as the past three variables.
The major difference is in the cleaning step: the inscriptions often contain
editorial marks which must be removed before any analysis.

The list of marks to be removed will be updated as more arise.


```{r GRAB TEXT, eval=FALSE, include=TRUE}
### GRAB TEXT ------------------------------------------------------------------
#textpage > div.text > div.greek.text-nowrap.dblclk > table
text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("\\n", "", text)
  text <- gsub("\\d", "", text)
  text <- gsub("-", "", text)
  #ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text)
```

# Second Draft
The script currently prints the scraped data onto the console. This is fine when
working with a single inscription, but will quickly become unusable when
scraping more pages. The variables `IGbook`, `IGno`, `PHIno`, and `text` should
be redefined as lists. 

Since I'm planning ahead, I suppose I should wrap the scraping into functions.

## Designing the Loop
The Packard Humanities Reference Number is *also the identifying portion of the
web address!* Everything after the ampsersand in the above address, 
`https://inscriptions.packhum.org/text/1825?&bookid=3&location=1701`,
is added while navigating through the website's tree of variables
(volume, region, etc.). The script can therefore be built around a `for()` loop
which α) builds a new variable for each loop and β) checks to ensure the
resulting web address corresponds to a valid PHI Reference Number.

## The Function
The loop is built around a function, currently called `TEST()`, which captures
and organizes the data for us.

```{r FUNCTION TEST, eval=FALSE, include=TRUE}
TEST <- function(PHIno = 20) {
  link <- paste('https://inscriptions.packhum.org/text/', PHIno, sep="")
  page <- read_html(link)
  
  ### Text
  #textpage > div.text > div.greek.text-nowrap.dblclk > table
  text <- page %>% html_nodes('div.greek.text-nowrap.dblclk') %>% html_text()
  text <- gsub("[\\d\\[\\]]", "", text, perl=T)
  text <- gsub("(\\-\\n{1,})", "", text, perl=T)
  text <- gsub("\\n{1,}", "", text, perl=T)
  text <- gsub("\\s{1,}", " ", text, perl=T)
  # ̣COMBINING DOT BELOW Unicode: U+0323, UTF-8: CC A3
  text <- gsub("̣", "", text, perl=T)
  text <- gsub("#", "", text, perl=T)
  # ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  text <- gsub("ℎ", "h", text, perl=T)
  
  ### IGBook
  #textpage > div.hdr2 > span.fullref > a
  IGbook <- page %>% html_nodes("span.fullref > a") %>% html_text()
  IGbook <- gsub("\\n", "", IGbook)
  
  ### IGNo
  IGno <- page %>% html_nodes("span.fullref > span") %>% html_text()
  IGno <- gsub("\\n", "", IGno)
  
  # Atm keeping for debugging
  # print(link)
  # print(text)
  # print(IGbook)
  # print(IGno)
  # print(PHIno)
  
  # Make entry to be returned
  entry <- c(IGbook, IGno, PHIno, text, link)
  return(as.list(entry))
  
  # Atm keeping for debugging
  # rm(link)
  # rm(text)
  # rm(IGbook)
  # rm(IGno)
  # rm(PHIno)
}
```

This essentially encapsulates the contents of the first draft. The main
differences are in the formatting changes (more notational marks to be removed)
and how it handles the output. Rather than printing off variables to the
terminal, the function creates a vector called `entry`, which it returns as a
list for later processing. The target `PHIno` is now  also identified as an
argument of the function. The default `PHIno` is 20 (IG I^3 20), because it's an
eyesore and I'm a masochist. Or because I slammed it in at random. WHo's to say.

There are also a few test lines at the bottom that use the `TEST()` function in
a `for()` loop to gather the first 100 inscriptions (by `PHIno`) from the
PackHum website.

```{r FIRST 100, eval=FALSE, include=TRUE}
# df[row, column]
test_df <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_df[1,] <- TEST(1)
test_df[2,] <- TEST(2)
test_df[3,] <- TEST(3)

for (i in c(1:100)) {
  test_df[i,] <- TEST(i)
}
```

```{r DISPLAY FIRST 100, echo=FALSE}
head(test_df)
```

## Bad Links
Now, trying to scan a non-existent page will obviously break things. An example
from the draft:

```{r BAD LINKS}
# testing bad links ------------------------------------------------------------
test_prime <- tibble(Book = NA, No = NA, Ref = NA, Text = NA, Link = NA)
test_prime[1,] <- TEST(0)
```

Two solutions come to mind. 

I could create a list of hypothetical selectors between 1 and the presumed
highest `PHIno`, which I then systematically "ping" to find the rejects. Running
`TEST(0)` will only return two outputs: the `IGbook` and the `link`.

```{r BAD OUTPUTS}
TEST(0)
```

This means there is certainly a path available where I run a `for()` loop, which
tests all selectors between 1 and *n* and sets their `IGbook` output into a
tibble. From there, any 0's can be dropped.

There is also the option where I manually define the ranges which I intend to
gather. This would avoid *scanning the Inscriptiones Graecae in its entirety*,
but would raise the chances of missing potential data.

---

After some thought, I'm of the opinion that I should use a function for link
validation. Assuming the code works, a programmatic approach should
outperform a manual search by every metric. This makes sense! It's a repetitive
task that would normally take many long hours to complete. It's everything a
computer excels at!

What's more, the output of the loop could be used as the limiter for any
`for()` loops, improving the function's control and reproducibility.


<style type="text/css">
.tg  {border-collapse:collapse;border-color:#93a1a1;border-spacing:0;margin:0px auto;}
.tg td{background-color:#fdf6e3;border-color:#93a1a1;border-style:solid;border-width:1px;color:#002b36;
  font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{background-color:#657b83;border-color:#93a1a1;border-style:solid;border-width:1px;color:#fdf6e3;
  font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<table class="tg">
<thead>
  <tr>
    <th class="tg-0lax"></th>
    <th class="tg-baqh">Automated</th>
    <th class="tg-baqh">Manual</th>
    <th class="tg-0lax">Assuming the code works …</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Time (Coding)</td>
    <td class="tg-baqh">-</td>
    <td class="tg-baqh">+</td>
    <td class="tg-0lax">Manual isn't coded</td>
  </tr>
  <tr>
    <td class="tg-0lax">Time (Search)</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Automated does it for you</td>
  </tr>
  <tr>
    <td class="tg-0lax">Output as Obj.</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Output can be used as `i`</td>
  </tr>
  <tr>
    <td class="tg-0lax">Thematic</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Paper is about programming</td>
  </tr>
  <tr>
    <td class="tg-0lax">Missed Links</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code will try every possible option</td>
  </tr>
  <tr>
    <td class="tg-0lax">Volume per TIme</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code will scan more in same time</td>
  </tr>
  <tr>
    <td class="tg-0lax">False pos/neg</td>
    <td class="tg-baqh">+</td>
    <td class="tg-baqh">-</td>
    <td class="tg-0lax">Code won't mistake a 0 for other `n`</td>
  </tr>
</tbody>
</table>


The matter of time brings up another point. Currently, I have no estimate for
how long anything will take to run. While this isn't a problem for smaller code
chunks, it may become a major factor in the full-scale scraper/miner. My laptop
isn't built to handle that kind of network or memory load, and I can't use the
GAI servers without my Dell, so this *needs to work* when I scale up. I
literally cannot run this repeatedly, I just do not have that kind of time on my
hands.

Time benchmarking is a faster fix, so that'll come first.

## Time Benchmarks
There are two options for measuring time elapsed during execution.

    1       Sys.time()
    2       system.time(FUNC())

`Sys.time()` returns the current time.
```{r sys.time()}
Sys.time()
```

`system.time(FUNC())` returns the execution time of some arbitrary function
FUNC(). It looses the output of the argument.

```{r system.time()}
system.time(TEST(200))
```

Current plan. Capture a timestamp at the beginning of execution. Capture another
at the end. Find the difference!
```{r Time Example}
# Capture first time stamp
test_time_a <- Sys.time()

# Wait for 20 seconds
Sys.sleep(5)

# Capture last time stamp
test_time_b <- Sys.time()

# Find difference
test_time_b - test_time_a
```

Now, adding `Sys.time()` directly to the function seems unnecessary. I don't
want to complicate things by adding more outputs to a function, plus I don't
really care how long it takes to capture the individual page. I'm more
interested in how long it takes to complete the full routine. Benchmarking
should then take place at the highest possible level.

## Time Estimating
I can, however, use `system.time()` in a loop to estimate how long a full-scale
execution might look. I can't do it quite yet — I don't have the upper bounds
that I'm estimating for — but I can keep it in my pocket.

# Third Draft
The project having been delayed for some time now, I find myself *overwhelmingly
confused by my lack of organization* (shocker)

Here's a rough outline of how the code should break down.

```{r Scraper Outline, eval=FALSE}
#phi_no is known in advance, or used to iterate the loop
#no need to define it, just add it into the mix during MakeEntry()
#idk what to do about the header yet so
Scrape(phi_no=1, dirty=F, remove_accent=T) =>

  # Find page for scraping
  page <- MakePage(phi_no)
  
  # Scrape relevant data + identifying materials
  text <- ReadText(page)
  ig_book <- ReadBook(page)
  ig_no <- ReadNo(page)
  header <- ReadHeader(page)
  
  # Clean data of editorial marks
  if (dirty=F) {
    text <- Clean(text)
  }
  
  # Clean data of accentual marks
  if (remove_accent=T) {
    no_accent <- RemoveAccent(text)
  } else {
    no_accent <- NA
  }
  
  # Make entry to be passed to tibble `data`
  entry <- MakeEntry(ig_book, ig_no, phi_no, text, duplicate, link)
  Return(entry)
  
# ---------------------------------------------------------
# Tibble `data` to be processed
data <- tibble(ig_book = NA, ig_no = NA, phi_no = NA, text = NA, duplicate = NA, link = NA)

# --------------------------------------------------------- 
# Master loop which creates the tibble
CollectData() =>
  for (i in c(1:n)) {
    data[i,] <- Scrape(phi_no, dirty=F, duplicate=T)
  }
```


Okay it's going to take 13 hours to run. So. Let's run a test scrape.

I'm going to try and grab all of Book 7...

Hey it worked!

## Headers
The script needs the ability to determine the location of an inscription and its
date range. To do so, the script first grabs the "header" from each page, then
cleans it of editorial marks (but not numbers!)

```{r HEADERS, eval=FALSE, include=TRUE}
ReadHeader <- function(page) {
  header <- page %>% html_nodes('div.tildeinfo.light') %>% html_text()
  return(header)
}

CleanHeader <- function(header) {
  header <- gsub("[\\[\\]]", "", header, perl=T)
  header <- gsub("(\\-\\n{1,})", "", header, perl=T)
  header <- gsub("\\n{1,}", " ", header, perl=T)
  header <- gsub("\\s{1,}", " ", header, perl=T)
  # ̣COMBINING DOT BELOW Unicode: U+0323, UTF-8: CC A3
  header <- gsub("̣", "", header, perl=T)
  header <- gsub("#", "", header, perl=T)
  # ℎ PLANCK CONSTANT Unicode: U+210E, UTF-8: E2 84 8E
  header <- gsub("ℎ", "h", header, perl=T)
  return(header)
}
```

The editors made the headers easy for a person to read, but they are not nearly
standardized enough for a computer. However, each header generally has the two
most important pieces of information: the location, and the date(s).

### Locations
Locations are easy enough to find. I am targeting known ranges in the IG for my
data, so I won't need to worry too much about errant locations emerging in the
data set. I can therefore make a variable `cities` which contains a RegEx string
that will identify the key cities (and only the key cities). I can then pass
this to `str_extract()`, and return the output as the variable `location`. This
method does hamper modularity, but it fits for the scope of the project.

Some cities are sometimes abbreviated in the header. The function
`ReadLocation` accordingly begins with a `gsub` command that spells out the
abbreviated city name.

```{r GRABBING LOCATIONS, eval=FALSE, include=TRUE}
cities <- "(Megara|Pagae|Aegosthena|Oropus|Tanagra|Eleusis|Athens)"

ReadLocation <- function(header) {
  header <- gsub("Ath.", "Athens", header, ignore.case = T, perl = T)
  location <- str_extract(header, cities)
  return(location)
}
```

So far, I have only seen the abbreviation "Ath." for "Athens", and so I have
only programmed for that one specifically.

### Dates
Dates are rather more difficult. The IG uses convenient shorthands — before 
3rd c. BC, Roman Era, 500-510, etc. — which are legible for a person but
confusing in their variety for a computer. I need to convert each into two
variables: `date_after` and `date_before`. I will then return each for their own
column.

First things first. The IG occasionally presents dates in "Xth c." format, which
is thoroughly incompatible with the rest of the data. I have written a short
function to "translate" these into usable data.

First, the script determines if a date is in the incorrect format.
The function `grepl()`, despite looking like it wants to `repl`ace something,
only actually searches for a pattern, then returns a `TRUE/FALSE` result
depending on whether or not it finds that pattern. If the search returns
`TRUE`, then the function captures the number from that date and stores it as
the variable `cen` (`cen`tury). The function then adds two trailing zeroes, and
subtracts a hundred to make a usable date.

Century dates are often described as "early", "middle"/"mid", or "late."
In my testing, I have almost unanimously seen a modifier followed by a single
date identifier. Therefore, if a modifier is present, the code returns a single
date. If no modifier is present, the code returns the entire century as a date
range (YYY-YYY). 

If the function notices one of these modifiers, then it adds some time onto the
date. For an "early" date, it adds nothing. For a "mid" date, it adds 50 years.
For a "late" date, 99 years. 

```{r TRANSLATE CENTURIES, eval=FALSE, include=TRUE}

TranslateCentury <- function(header) {
  grepl("(\\d+)(?=\\w{2} c.)", header, perl = T)
  cen_start <- str_extract(header, "(\\d+)(?=\\w{2} c.)")
  cen_start <- paste(cen_start, "00", sep="") %>%
    as.integer()
  
  cen_end <- cen_start - 99
  
  if (grepl("(?<=early )(\\d+)(?=\\w{2} c.)", header, perl = T)) {
    return(cen_start)
  } else if (grepl("(?<=late )(\\d+)(?=\\w{2} c.)", header, perl = T)) {
    cen_start <- cen_start - 99
    return(cen_start)
  } else if (grepl("(?<=mid )(\\d+)(?=\\w{2} c.)", header, perl = T)) {
    cen_start <- cen_start - 50
    return(cen_start)
  } else {
    cen <- paste(cen_start, cen_end, sep="-")
    return(cen)
  }

}
```

The code for both dates are paralleled. I have built each around an `if-else`
function, which searches for possible dates in a number of formats (currently 2,
third in prog).

```{r GET DATES, eval=FALSE, include=TRUE}
ReadDateAfter <- function(header) {
  if (grepl("\\d+-\\d+", header, perl = T)) {
    date_after <- str_extract(header, "\\d+(?=-)")
  } else if (grepl("(?<=after )\\d+", header, perl = T)) {
    date_after <- str_extract(header, "(?<=after )\\d+")
  }  else if (grepl("(?<=— )(\\d+)(?= BC)", header, perl = T)) {
    date_after <- str_extract(header, "(?<=— )(\\d+)(?= BC)")
  } else {date_after <- NA}
  return(date_after)
}

ReadDateBefore <- function(header) {
  if (grepl("\\d+-\\d+", header, perl = T)) {
    date_before <- str_extract(header, "(?<=-)\\d+")
  } else if (grepl("(?<=before )\\d+", header, perl = T)) {
    date_before <- str_extract(header, "(?<=before )\\d+")
  } else if (grepl("(?<=— )(\\d+)(?= BC)", header, perl = T)) {
    date_before <- str_extract(header, "(?<=— )(\\d+)(?= BC)")
  } else {date_before <- NA}
  return(date_before)
}
```


The pattern grepl `\\d+-\\d+` searches for two digits (`\\d+`) separated by
a dash `-`. This is the first test used in both `ReadDateAfter` and
`ReadDateBefore`. If a simple date range is found, then `ReadDateAfter` grabs
the number before the dash using a *positive lookahead* `(?=<char>)` — i.e., the
code will only grab the number if it is followed by the character `<char>`
(here, a dash `-`). `ReadDateBefore` performs a similar function to simple
dates, but grabs the digit after the dash, using a *positive lookbehind*
`(?<=<char>)`. There can be multiple different numbers in the heading, so I
have added lookarounds (-ahead, -behind) to make sure that the code returns the
proper variables.

If the first test fails — i.e., there are no simple dates in YYY-YYY format —
then the functions look for either "after " or "before ", then grabs the
following numbers. It is *excruciatingly* important that the space between
"after/before" and the date is included in the lookaround, or else RegEx will
expect to see the phrase crammed right against the date. *It cannot interpret
the search expression, it must receive exactly what you want to search for!*

### Encoding
Now, Regular Expression does not meld well with Greek letters. This is to be
expected: after all, the system was designed in the 50's, well before computers
were able to handle anything out of the Latin ASCII table. This may seem damning
on the surface, but the fix is simple if somewhat inelegant. 

I'm going to copy Perseus here. All text will be transcribed into the Latin
alphabet during processing, then converted back to the Greek alphabet for
presentation. For simplicity, I am ripping the encoding scheme from the Perseus
project.

<img src="http://www.perseus.tufts.edu/img/keyCaps.gif" alt="Italian Trulli">

The key difference here is that accents are already stripped away. Plus, I have
included a few characters that Perseus either avoids or doesn't publicize the
code for. Digamma, for instance, is already coded into the Perseus Project as
"v/V".
